{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "DataPreparing.ipynbで準備したデータ\n",
    "\n",
    "s3://###default bucket###/xgboost-churn-stepfunctions/xgboost-churn/churn.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# データをS3から取得\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file('xgboost-churn-stepfunctions/xgboost-churn/churn.txt', 'churn.txt')\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記セルを実行して、SageMaker Python SDK Version が 1.xx.x の場合、以下のセルのコメントアウトを解除してから実行してください。実行が完了したら、上にあるメニューから [Kernel] -> [Restart kernel] を選択してカーネルを再起動してください。\n",
    "\n",
    "再起動が完了したら、このノートブックの一番上のセルから再度実行してください。その場合、以下のセルを実行する必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --quiet \"sagemaker==2.16.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ライブラリのセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "churn = pd.read_csv('./churn.txt')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "データをみると 3,333 行のデータしかなく、現在の機械学習の状況から見ると、やや小さいデータセットです。各データのレコードは、ある米国の携帯電話会社の顧客のプロフィールを説明する21の属性からなります。その属性というのは、\n",
    "\n",
    "- State: 顧客が居住している米国州で、2文字の省略形で記載されます (OHとかNJのように)\n",
    "- Account Length: アカウントが利用可能になってからの経過日数\n",
    "- Area Code: 顧客の電話番号に対応する3桁のエリアコード\n",
    "- Phone: 残りの7桁の電話番号\n",
    "- Int’l Plan: 国際電話のプランに加入しているかどうか (yes/no)\n",
    "- VMail Plan: Voice mail の機能を利用しているかどうか (yes/no)\n",
    "- VMail Message: 1ヶ月の Voice mail のメッセージの平均長\n",
    "- Day Mins: 1日に通話した時間(分)の総和\n",
    "- Day Calls: 1日に通話した回数の総和\n",
    "- Day Charge: 日中の通話にかかった料金\n",
    "- Eve Mins, Eve Calls, Eve Charge: 夜間通話にかかった料金\n",
    "- Night Mins, Night Calls, Night Charge: 深夜通話にかかった料金\n",
    "- Intl Mins, Intl Calls, Intl Charge: 国際通話にかかった料金\n",
    "- CustServ Calls: カスタマーサービスに電話をかけた回数\n",
    "- Churn?: そのサービスから離反したかどうか (true/false)\n",
    "\n",
    "最後の属性 Churn? は目的変数として知られ、MLのモデルで予測する属性になります。目的変数は2値 (binary) なので、ここで作成するモデルは2値の予測を行います。これは2値分類といわれます。\n",
    "\n",
    "それではデータを詳しく見てみます。\n",
    "\n",
    "まずはカテゴリデータごとにデータの頻度をみてみます。カテゴリデータは、State, Area code, Phone, Int’l Plan, VMail Plan, Churn?で、カテゴリを表す文字列や数値がデータとして与えられているものです。pandasではある程度自動で、カテゴリデータを認識し、objectというタイプでデータを保存します。以下では、object 形式のデータをとりだして、カテゴリごとの頻度を表示します。\n",
    "\n",
    "また describe()を利用すると各属性の統計量を一度に見ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(churn.describe())\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを見てみると以下のことに気づくと思います。\n",
    "\n",
    "- State の各頻度はだいたい一様に分布しています。\n",
    "- Phone はすべて同じ数値になっていて手がかりになりそうにありません。この電話番号の最初の3桁はなにか意味がありそうですが、その割当に意味がないのであれば、使うのは止めるべきでしょう\n",
    "- たった14%の顧客が離反しているので、インバランスなデータと言えるでしょうが、そこまで極端ではありません\n",
    "- 数値的な特徴量は都合の良い形で分布しており、多くは釣り鐘のようなガウス分布をしています。ただ、VMail Messageは例外です。\n",
    "- Area code は数値データとみなされているようなので、非数値に変換しましょう\n",
    "\n",
    "さて、実際にPhoneの列を削除して、Area codeを非数値に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは次に各属性の値を、目的変数の True か False か、にわけて見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn?':\n",
    "        display(pd.crosstab(index=churn[column], columns=churn['Churn?'], normalize='columns'))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ分析の結果から、離反する顧客について、以下のような傾向が考えられます。\n",
    "\n",
    "- 地理的にもほぼ一様に分散している\n",
    "- 国際通話を利用している\n",
    "- VoiceMailを利用していない\n",
    "- 通話時間で見ると長い通話時間と短い通話時間の人に分かれる\n",
    "- カスタマーサービスへの通話が多い (多くの問題を経験した顧客ほど離反するというのは理解できる)\n",
    "\n",
    "加えて、離反する顧客に関しては、Day Mins と Day Charge で似たような分布を示しています。しかし、話せば話すほど、通常課金されるので、驚くことではないです。もう少し深く調べてみましょう。corr() を利用すると相関係数を求めることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(churn.corr())\n",
    "pd.plotting.scatter_matrix(churn, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いくつかの特徴は互いに100%の相関をもっています。このような特徴があるとき、機械学習のアルゴリズムによっては全くうまくいかないことがあり、そうでなくても結果が偏ったりしてしまうことがあります。これらの相関の強いペアは削除しましょう。Day Mins に対する Day Charge、Night Mins に対する Night Charge、Intl Mins に対する Intl Charge を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまででデータセットの前処理は完了です。これから利用する機械学習のアルゴリズムを決めましょう。前述したように、数値の大小 (中間のような数値ではなく)で離反を予測するような変数を用意すると良さそうです。線形回帰のようなアルゴリズムでこれを行う場合は、複数の項（もしくはそれらをまとめた項）を属性として用意する必要があります。\n",
    "\n",
    "そのかわりに、これを勾配ブースティング木 (Gradient Boosted Tree)を利用しましょう。Amazon SageMaker は、マネージドで、分散学習が設定済みで、リアルタイム推論のためのホスティングも可能な XGBoost コンテナを用意しています。XGBoost は、特徴感の非線形な関係を考慮した勾配ブースティング木を利用しており、特徴感の複雑な関連性を扱うことができます。\n",
    "\n",
    "Amazon SageMaker の XGBoostは、csv または LibSVM 形式のデータを学習することができます。ここでは csv を利用します。csv は以下のようなデータである必要があります。\n",
    "\n",
    "- 1列目が予測対象のデータ\n",
    "- ヘッダ行はなし\n",
    "\n",
    "まずはじめに、カテゴリ変数を数値データに変換する必要があります。get_dummies() を利用すると数値データへの変換が可能です。\n",
    "\n",
    "そして、Churn?_Trueのデータを最初の列にもってきて、Churn?_False., Churn?_True.のデータを削除した残りのデータをconcatenate (連結) します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで学習用、バリデーション用、テスト用データにわけましょう。これによって overfitting (学習用データには精度が良いが、実際に利用すると制度が悪い、といった状況) を回避しやすくなり、未知のテストデータに対する精度を確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "test_data.to_csv('test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "input_train = sagemaker_session.upload_data(path='train.csv', key_prefix='xgboost-churn-stepfunctions/xgboost-churn')\n",
    "input_validation = sagemaker_session.upload_data(path='validation.csv', key_prefix='xgboost-churn-stepfunctions/xgboost-churn')\n",
    "input_test = sagemaker_session.upload_data(path='test.csv', key_prefix='xgboost-churn-stepfunctions/xgboost-churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_train)\n",
    "print(input_validation)\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理をpythonスクリプトに書き出す"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "今回のデータに必要な前処理\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "churn = pd.read_csv('./churn.txt')\n",
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)\n",
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)\n",
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)\n",
    "\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# boto3の機能を使ってリポジトリ名に必要な情報を取得する\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "print(region)\n",
    "print(account_id)\n",
    "ecr_repository = 'xgboost-churn-processing'\n",
    "tag = ':latest'\n",
    "nlpsample_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "# リポジトリの作成\n",
    "# すでにある場合はこのコマンドは必要ありません。\n",
    "!aws ecr create-repository --repository-name $ecr_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t xgboost-churn-processing ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker imageをecrにpush\n",
    "!docker tag {ecr_repository + tag} $nlpsample_repository_uri\n",
    "!docker push $nlpsample_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### localで実行の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "role = get_execution_role()\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri='%s.dkr.ecr.ap-northeast-1.amazonaws.com/%s:latest' % (account_id, ecr_repository),\n",
    "    role=role,\n",
    "    command=['python3'],\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(code='processing.py',\n",
    "    inputs=[ProcessingInput(\n",
    "        source='churn.txt',\n",
    "        destination='/opt/ml/processing/input')],\n",
    "        outputs=[\n",
    "        ProcessingOutput(source='/opt/ml/processing/output/data')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
